% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gR2.R
\name{gR2}
\alias{gR2}
\title{Generalized R-squared for GLM Models}
\usage{
gR2(
  full_glm,
  null_glm = NULL,
  terms = NULL,
  normalize = FALSE,
  adjusted = FALSE,
  algorithm = "auto",
  algorithm.control = list(n_exact = 15, thresholds = c(-0.1, 0, 0.1), n_random = max(1,
    13 + log(1/nrow(model.matrix(full_glm)))), max_iter = 1000, topK = max(10, min(100,
    length(nrow(model.matrix(full_glm)))/10)), tol = 1e-12, patience = 10)
)
}
\arguments{
\item{full_glm}{A fitted GLM object of class `glm` (the full model).}

\item{null_glm}{A fitted GLM object of class `glm` (the null model). If `NULL` (default),
uses an empty model (intercept-only if intercept is present, otherwise no predictors).}

\item{terms}{A character vector of variable names or a formula specifying
the additional terms in the full model compared to the null. If provided,
this overrides `null_glm` and the null model is refitted excluding these terms.}

\item{normalize}{FALSE by default.}

\item{algorithm.control}{`list` of control parameters:
\itemize{
  \item `n_exact` Integer specifying the sample size threshold for using exact
    methods (brute force). Default is 15.
  \item `thresholds` Numeric vector of threshold values for multi-start initialization.
  \item `n_random` Integer number of random starts for multi-start optimization.
  \item `max_iter` Integer maximum number of iterations per start.
  \item `topK` Integer number of top candidates to consider at each iteration.
  \item `tol` Numeric tolerance for convergence.
  \item `patience` Integer number of iterations without improvement before stopping.
}}

\item{algorith}{`"auto"` by default. It choose between `"intercept_only"`, `"brute_force"` and `"multi_start"`}
}
\value{
If normalize==FALSE: A numeric value representing the generalized R-squared measure.
If normalize==TRUE: A list with components:
  \item{R2}{The generalized R-squared coefficient for the set of terms}
  \item{R2_n}{The normalized generalized R-squared coefficient}
  \item{algorithm}{The algorithm used to compute the maximum R-squared}
  \item{terms_tested}{The names of the terms included in the test}
}
\description{
Computes the generalized R-squared measure for nested generalized linear models.
The generalized R-squared measures the proportion of "variance" explained by
the additional predictors in the full model compared to the null model.
}
\details{
The generalized R-squared is computed as:

\deqn{gR^2 = \frac{Y_r^\top X_r (X_r^\top X_r)^{-1} X_r^\top Y_r}{Y_r^\top Y_r}}

where:
\itemize{
  \item \eqn{Y_r = V^{-1/2}(Y - \hat{\mu}_0)} is the standardized residual vector from the null model
  \item \eqn{X_r = (I - H)W^{1/2}X} is the residualized additional predictors matrix
  \item \eqn{H} is the hat matrix for the null model
  \item \eqn{W = DV^{-1}D} is the weight matrix
}

This measures the proportion of the standardized residual sum of squares explained
by the additional predictors in the full model.

The normalized generalized R-squared is computed as:
\deqn{
R^2_n = \frac{R^2}{R^2_{\max}}
}
where \eqn{R^2_{\max}} is the maximum possible R-squared value for the specified
set of terms.

Different algorithms are used based on sample size:
\itemize{
  \item For small samples (\eqn{n \leq n_{\text{exact}}}), brute force search finds the exact maximum
  \item For larger samples, a greedy multi-start algorithm finds approximate maximum
}
}
\examples{
set.seed(1)
dt=data.frame(X=rnorm(20),
   Z=factor(rep(LETTERS[1:3],length.out=20)))
dt$Y=rpois(n=20,lambda=exp(dt$Z=="C"))
mod=glm(Y~Z+X,data=dt,family="poisson")
summary(mod)

# Compute generalized partial correlations for all variables
(results <- gR2(mod))
# equivalent to
mod0=glm(Y~0,data=dt,family="poisson")
(results <- gR2(mod, mod0))
(results <- gR2(mod, mod0,normalize=TRUE))

# Compute for specific variables only
(results <- gR2(mod,terms = c("X","Z")))
(results <- gR2(mod,terms = c("X","Z")))


set.seed(123)
dt <- data.frame(X = rnorm(20),
                 Z = factor(rep(LETTERS[1:3], length.out = 20)))
dt$Y <- rbinom(n = 20, prob = plogis((dt$Z == "C") * 2), size = 1)
mod <- glm(Y ~ Z + X, data = dt, family = binomial)

# Compute generalized partial correlations for all variables
(results <-  gR2(mod,normalize=TRUE))
# equivalent to
mod0=glm(Y~1,data=dt,family=binomial)
(results <-  gR2(mod, mod0,normalize=TRUE))

# Compute for specific variables only
(results <-  gR2(mod,terms = c("X","Z"),normalize=TRUE))


# Compute generalized (non partial!) correlations for all variables
mod <- glm(Y ~ X, data = dt, family = binomial)
(results <-  gR2(mod,normalize=TRUE))
# note the difference:
(results <-  gR2(mod,normalize=TRUE,algorithm="intercept_only"))
# Despite the result is the same in this case,
# the multi_start algorithm does not ensure exactness (while intercept_only and brute_force do)
(results <-  gR2(mod,normalize=TRUE,algorithm="multi_start"))

}
\author{
Livio Finos and Paolo Girardi
}
